{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2TMMLN_v49y"
   },
   "source": [
    "# **Web Scraper and Data Processor with LLM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNMdRBMRwe1m"
   },
   "source": [
    "This notebook demonstrates how to:\n",
    "1. **Scrape web content** from specified URLs.\n",
    "2. **Extract and clean** the body content from the HTML.\n",
    "3. **Split the content** into manageable chunks.\n",
    "4. **Use a Language Model (LLM)** to parse and extract specific information based on a given prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08c8ZnW0wDZg"
   },
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4 langchain_ollama langchain_core httpx pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFg0qy2XwqZ5"
   },
   "source": [
    "## 1. Importing Libraries\n",
    "\n",
    "We will start by importing the necessary libraries for web scraping, data processing, and interacting with the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PzWLepHnts5p"
   },
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import requests  # For making HTTP requests\n",
    "from bs4 import BeautifulSoup  # For parsing the website's HTML content\n",
    "from langchain_ollama import OllamaLLM  # Importing the LLM interface\n",
    "from langchain_core.prompts import ChatPromptTemplate  # For templating the prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5wBEqwPw-uz"
   },
   "source": [
    "## 2. Define the LLM Integration and Processing Functions\n",
    "\n",
    "Template and LLM Initialization\n",
    "\n",
    "Define the prompt template and initialize the LLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "JT_UeCGstTf9"
   },
   "outputs": [],
   "source": [
    "# Define the prompt template for extracting specific information from web content\n",
    "prompt_template = (\n",
    "    \"You are tasked with extracting specific information from the following text content: {dom_content}. \"\n",
    "    \"Please follow these instructions carefully: \\n\\n\"\n",
    "    \"1. **Extract Information:** Only extract the information that directly matches the provided description: {parse_description}. \"\n",
    "    \"2. **No Extra Content:** Do not include any additional text, comments, or explanations in your response. \"\n",
    "    \"3. **Empty Response:** If no information matches the description, return an empty string ('').\"\n",
    "    \"4. **Direct Data Only:** Your output should contain only the data that is explicitly requested, with no other text.\"\n",
    ")\n",
    "\n",
    "# Initialize the LLaMA3 model (Ensure that the server is running on localhost and port is correct)\n",
    "llama_model = OllamaLLM(model=\"llama3.1\", host=\"localhost\", port=11434)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9wRTG67xKkH"
   },
   "source": [
    "## 3. Define the SmartScraper Class\n",
    "This class will handle the scraping, content extraction, cleaning, and LLM processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "TeIdsEfetdxi"
   },
   "outputs": [],
   "source": [
    "# Define the SmartScraper class that integrates web scraping and LLM-based parsing\n",
    "class SmartScraper:\n",
    "    def __init__(self, url, llm_model, prompt):\n",
    "        \"\"\"\n",
    "        Initialize the SmartScraper object with the URL, LLM model, and prompt.\n",
    "        :param url: URL of the website to scrape.\n",
    "        :param llm_model: Initialized LLaMA model object.\n",
    "        :param prompt: Prompt template for LLaMA model.\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        self.llm_model = llm_model\n",
    "        self.prompt_template = prompt\n",
    "\n",
    "    def scrape_website(self):\n",
    "        \"\"\"\n",
    "        Fetch the website content from the provided URL.\n",
    "        :return: HTML content of the webpage or empty string if request fails.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Connecting to the website...\")\n",
    "            response = requests.get(self.url)\n",
    "            response.raise_for_status()  # Check for HTTP errors\n",
    "            print(\"Successfully connected and retrieved content.\")\n",
    "            return response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred while fetching the URL: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def extract_body_content(self, html_content):\n",
    "        \"\"\"\n",
    "        Extract the main body content from the HTML of the webpage.\n",
    "        :param html_content: Full HTML content of the webpage.\n",
    "        :return: Extracted body content as a string.\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        body_content = soup.body\n",
    "        if body_content:\n",
    "            return str(body_content)\n",
    "        return \"\"\n",
    "\n",
    "    def clean_body_content(self, body_content):\n",
    "        \"\"\"\n",
    "        Clean the body content by removing scripts, styles, and unnecessary whitespace.\n",
    "        :param body_content: HTML content of the body section.\n",
    "        :return: Cleaned plain text content.\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(body_content, \"html.parser\")\n",
    "\n",
    "        # Remove script and style elements\n",
    "        for script_or_style in soup([\"script\", \"style\"]):\n",
    "            script_or_style.extract()\n",
    "\n",
    "        # Get cleaned text, stripping excess whitespace and newlines\n",
    "        cleaned_content = soup.get_text(separator=\"\\n\")\n",
    "        cleaned_content = \"\\n\".join(line.strip() for line in cleaned_content.splitlines() if line.strip())\n",
    "\n",
    "        return cleaned_content\n",
    "\n",
    "    def split_content_into_chunks(self, content, max_length=6000):\n",
    "        \"\"\"\n",
    "        Split the content into smaller chunks to ensure it fits within model input limits.\n",
    "        :param content: The full cleaned content of the webpage.\n",
    "        :param max_length: Maximum allowed length per chunk.\n",
    "        :return: List of content chunks.\n",
    "        \"\"\"\n",
    "        return [content[i:i + max_length] for i in range(0, len(content), max_length)]\n",
    "\n",
    "    def parse_with_llama(self, content_chunks, parse_description):\n",
    "        \"\"\"\n",
    "        Use the LLaMA model to parse and extract specific information from the content.\n",
    "        :param content_chunks: List of content chunks.\n",
    "        :param parse_description: The description of the information to extract.\n",
    "        :return: Parsed result as a string.\n",
    "        \"\"\"\n",
    "        # Initialize the prompt template\n",
    "        prompt = ChatPromptTemplate.from_template(self.prompt_template)\n",
    "\n",
    "        # Combine prompt template with LLM model\n",
    "        chain = prompt | self.llm_model\n",
    "\n",
    "        # Collect parsed results\n",
    "        parsed_results = []\n",
    "\n",
    "        for i, chunk in enumerate(content_chunks, start=1):\n",
    "            response = chain.invoke({\"dom_content\": chunk, \"parse_description\": parse_description})\n",
    "            print(f\"Processing chunk {i} of {len(content_chunks)}\")\n",
    "            parsed_results.append(response)\n",
    "\n",
    "        return \"\\n\".join(parsed_results)\n",
    "\n",
    "    def run(self, parse_description):\n",
    "        \"\"\"\n",
    "        Run the full scraping and parsing workflow.\n",
    "        :param parse_description: The description of the data to extract.\n",
    "        :return: Parsed result or error message.\n",
    "        \"\"\"\n",
    "        # Step 1: Scrape the website content\n",
    "        dom_content = self.scrape_website()\n",
    "        if not dom_content:\n",
    "            return \"Failed to retrieve content from the website.\"\n",
    "\n",
    "        # Step 2: Extract and clean body content\n",
    "        body_content = self.extract_body_content(dom_content)\n",
    "        cleaned_content = self.clean_body_content(body_content)\n",
    "\n",
    "        # Step 3: Split content into manageable chunks\n",
    "        content_chunks = self.split_content_into_chunks(cleaned_content)\n",
    "\n",
    "        # Step 4: Parse the content using LLaMA and the provided prompt\n",
    "        parsed_result = self.parse_with_llama(content_chunks, parse_description)\n",
    "        return parsed_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "on3Azkoaxuac"
   },
   "source": [
    "## 4. Example Usage\n",
    "Define Sample URLs and Prompts\n",
    "\n",
    "Here we provide URLs and prompts to demonstrate the functionality of SmartScraper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "SJONidcUti3Q",
    "outputId": "c8f85578-3950-45ad-ac12-1a516703e8de"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define sample URLs and prompts\n",
    "urls = [\n",
    "    \"https://argentwork.com/\",\n",
    "    \"https://www.reservebar.com/\"\n",
    "]\n",
    "\n",
    "prompts = [\n",
    "    \"Extract all inventory related details in table format.\",\n",
    "    \"Extract all inventory related details in table format\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UvPE2Yvy39T"
   },
   "source": [
    "## 5. Create and Run the Scraper Instances\n",
    "\n",
    "Instantiate SmartScraper for each URL and prompt, then execute the scraping and parsing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "MNWswZPKx_HE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the website...\n",
      "Successfully connected and retrieved content.\n",
      "Processing chunk 1 of 2\n",
      "Processing chunk 2 of 2\n",
      "Results for https://argentwork.com/:\n",
      "Here are the extracted inventory-related details in a table format:\n",
      "\n",
      "**Inventory Details**\n",
      "\n",
      "| **Category** | **Description** |\n",
      "| --- | --- |\n",
      "| **Item Name** | Product/Service description |\n",
      "| **Quantity** | Number of units available |\n",
      "| **Unit Price** | Cost per unit |\n",
      "| **Total Value** | Total cost of all items in stock |\n",
      "| **Low Stock Threshold** | Alert level to reorder items |\n",
      "| **Reorder Point** | Quantity at which to automatically restock |\n",
      "| **Supplier Information** | Contact details and terms for suppliers |\n",
      "\n",
      "Let me know if you'd like me to add any other inventory-related columns!\n",
      "Here are the extracted inventory-related details in a table format:\n",
      "\n",
      "**Inventory Details**\n",
      "\n",
      "| **Category** | **Description** |\n",
      "| --- | --- |\n",
      "| **Current Inventory Level** | 500 units (as of today) |\n",
      "| **Product Variants** | Standard, Premium, and Customized products available. |\n",
      "| **Warehouse Location** | Main warehouse: XYZ Storage Facility, New York; Secondary warehouse: ABC Logistics Center, Los Angeles. |\n",
      "| **Inventory Types** | Raw Materials, Work-in-Progress (WIP), Finished Goods, and Scrap materials. |\n",
      "| **Stockroom Organization** | FIFO (First-In-First-Out) inventory management system used for stockroom organization. |\n",
      "| **Expiration Dates** | Expiration dates recorded for perishable goods to ensure timely replenishment and avoid spoilage. |\n",
      "| **Inventory Valuation** | Average Cost method used for inventory valuation, with regular adjustments made as needed. |\n",
      "| **Order Thresholds** | Automatic reordering triggered when stock levels fall below 20 units (for Standard products) and 50 units (for Premium products). |\n",
      "\n",
      "Let me know if you'd like me to add anything else!\n",
      "\n",
      "==================================================\n",
      "\n",
      "Connecting to the website...\n",
      "Successfully connected and retrieved content.\n",
      "Processing chunk 1 of 3\n",
      "Processing chunk 2 of 3\n",
      "Processing chunk 3 of 3\n",
      "Results for https://www.reservebar.com/:\n",
      "Here are the inventory-related details extracted in a table format:\n",
      "\n",
      "**Inventory Details**\n",
      "\n",
      "| **Category** | **Description** | **Unit Price** | **Quantity** | **Total Value** |\n",
      "| --- | --- | --- | --- | --- |\n",
      "| **Electronics** | Laptops (10) | $1,500.00 | 20 | $30,000.00 |\n",
      "|  | Smartphones (5) | $800.00 | 15 | $12,000.00 |\n",
      "|  | Tablets (8) | $400.00 | 25 | $10,000.00 |\n",
      "| **Fashion** | T-Shirts (50) | $20.00 | 100 | $2,000.00 |\n",
      "|  | Jeans (30) | $40.00 | 60 | $2,400.00 |\n",
      "|  | Shoes (20) | $80.00 | 40 | $3,200.00 |\n",
      "| **Home Goods** | Bedding (15) | $100.00 | 25 | $2,500.00 |\n",
      "|  | Kitchenware (10) | $50.00 | 30 | $1,500.00 |\n",
      "|  | Decorative Items (5) | $200.00 | 20 | $4,000.00 |\n",
      "\n",
      "**Inventory Summary**\n",
      "\n",
      "* Total Inventory Value: $65,300.00\n",
      "* Total Quantity: 325\n",
      "* Average Unit Price: $201.44\n",
      "\n",
      "Let me know if you'd like to add or modify any details!\n",
      "Since you didn't provide any specific information, I'll create a general inventory management table with common fields. Here are the extracted inventory-related details in a table format:\n",
      "\n",
      "**Inventory Management Details**\n",
      "\n",
      "| **Category** | **Description** |\n",
      "| --- | --- |\n",
      "| **Item ID** | Unique identifier for each item (e.g., product or stock) |\n",
      "| **Item Name** | Description of the item, including its type and characteristics |\n",
      "| **Quantity** | Current number of items in stock |\n",
      "| **Unit Price** | Cost per unit of the item |\n",
      "| **Total Value** | Total cost of all items in stock |\n",
      "| **Supplier** | Company or individual providing the item(s) |\n",
      "| **Minimum Quantity (MQ)** | Re-order point to maintain a minimum stock level |\n",
      "| **Maximum Quantity (MXQ)** | Maximum quantity allowed for each item |\n",
      "| **Re-Order Point** | Trigger point to re-stock items, based on usage and lead time |\n",
      "| **Lead Time** | Time taken by the supplier to deliver new stock |\n",
      "| **Warehouse Location** | Storage facility where the item is currently held |\n",
      "| **Inventory Status** | Current status of the inventory (e.g., in stock, out of stock, or pending) |\n",
      "\n",
      "Let me know if you'd like me to add any specific fields related to your use case!\n",
      "Here are the inventory-related details extracted into a table format:\n",
      "\n",
      "**Inventory Management Details**\n",
      "\n",
      "| **Category** | **Description** | **Quantity** | **Unit** |\n",
      "| --- | --- | --- | --- |\n",
      "| Raw Materials | Wood | 5000 | Units |\n",
      "| Raw Materials | Metal Sheets | 2000 | Pieces |\n",
      "| Finished Goods | Chairs | 1500 | Units |\n",
      "| Finished Goods | Tables | 1000 | Units |\n",
      "| Packaging Materials | Boxes (Small) | 30000 | Units |\n",
      "| Packaging Materials | Boxes (Large) | 15000 | Units |\n",
      "| Accessories | Screws | 100000 | Pieces |\n",
      "| Accessories | Nails | 50000 | Pieces |\n",
      "\n",
      "**Warehouse Details**\n",
      "\n",
      "| **Location** | **Inventory Value** | **Inventory Count** |\n",
      "| --- | --- | --- |\n",
      "| Warehouse A | $1,200,000 | 25000 units |\n",
      "| Warehouse B | $800,000 | 15000 units |\n",
      "\n",
      "**Reorder Levels**\n",
      "\n",
      "| **Item** | **Minimum Reorder Level** | **Maximum Reorder Level** |\n",
      "| --- | --- | --- |\n",
      "| Raw Materials (Wood) | 1000 units | 2000 units |\n",
      "| Finished Goods (Chairs) | 500 units | 1000 units |\n",
      "\n",
      "**Inventory History**\n",
      "\n",
      "| **Date** | **Item** | **Quantity Added** | **Quantity Removed** |\n",
      "| --- | --- | --- | --- |\n",
      "| 2022-02-01 | Raw Materials (Wood) | 1000 units | 0 units |\n",
      "| 2022-03-15 | Finished Goods (Chairs) | 500 units | 200 units |\n",
      "\n",
      "Please let me know if you'd like me to add or modify anything!\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example execution for each URL and prompt\n",
    "for url, prompt in zip(urls, prompts):\n",
    "    scraper = SmartScraper(url, llama_model, prompt)\n",
    "    result = scraper.run(prompt)\n",
    "    print(f\"Results for {url}:\")\n",
    "    print(result)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ns2dxX7NzA0L"
   },
   "source": [
    "## **Conclusion**\n",
    "This notebook demonstrates a complete workflow for scraping web content, cleaning and processing it, and using an LLM to extract specific information. Customize the URLs and prompts as needed for your use case.\n",
    "\n",
    "\n",
    "### Notes:\n",
    "- **Port Number**: Ensure the `port` specified in `OllamaLLM` is the correct one where the Ollama server is running.\n",
    "- **Server Connection**: Make sure the Ollama server is up and running before executing the notebook.\n",
    "- **Error Handling**: The `ConnectError` indicates that the connection to the Ollama server is failing; double-check the server address and port.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
